=  HDFS连接器
:keywords: anypoint studio, connectors, hdfs

社区

Hadoop分布式文件系统（HDFS）的Anypoint连接器用作应用程序之间的双向网关。它的源存储在 http://mulesoft.github.io/mule-hadoop-connector[HDFS连接器GitHub站点]中。

*Release Notes:* link:/release-notes/hdfs-connector-release-notes[HDFS连接器发行说明]

== 先决条件

要使用HDFS连接器，您必须具备以下条件：

* 正在运行的Apache Hadoop服务器。
*  Anypoint Studio社区版。

本文档假定您熟悉Mule， link:/mule-user-guide/v/3.7/anypoint-connectors[Anypoint连接器]和 link:/anypoint-studio/v/5/index[Anypoint Studio Essentials]。为提高您对Studio的熟悉度，请考虑完成一个或多个 link:/anypoint-studio/v/5/basic-studio-tutorial[Anypoint Studio教程]。此外，本页假定您对 link:/mule-user-guide/v/3.7/mule-concepts[骡流]和 link:/mule-user-guide/v/3.7/global-elements[骡全球元素]有基本的了解。

本文档描述了Anypoint Studio（Mule ESB的图形用户界面）上下文中的实现示例，还包括在XML编辑器中执行相同操作的配置详细信息。

== 兼容性

HDFS Hadoop连接器与以下设备兼容：

[%header,cols="2*"]
|===
一个|
应用程序/服务

 为|
版

| Mule运行时 | 3.6或更新版本
| Apache Hadoop  | 2.6.0或更高版本
|===

== 安装和配置

=== 在Anypoint Studio中安装HDFS连接器

. 在Anypoint Studio中，点击Studio任务栏中的Exchange图标。
. 点击Anypoint Exchange中的登录。
. 搜索连接器，然后单击安装。
. 按照提示安装连接器。

Studio有更新时，会在右下角显示一条消息，您可以单击该消息来安装更新。

=== 配置全局元素

要在Mule应用程序中使用HDFS连接器，您必须配置一个全局HDFS元素，供应用程序中的所有HDFS连接器使用（请阅读有关 link:/mule-user-guide/v/3.7/global-elements[全球元素]的更多信息）。

[tabs]
------
[tab,title="Studio Visual Editor"]
....
. Click the *Global Elements* tab at the base of the canvas, then click *Create*.
. In the *Choose Global Type* wizard, use the filter to locate and select *HDFS*, then click *OK*.
. Configure the parameters according to the table below.
+
image:hdfs_connector.png[hdfs_connector]
+
[%header,cols="30a,70a"]
|===
|Parameter |Description
|*Name* |Enter a name for the configuration to reference later.
|*NameNode URI* |Enter the host name or IP address of the master node of the Hadoop cluster.
|*Username* |Enter a Hadoop File System username.
|*Configuration Resources* |If you want to override the configuration resources of the Hadoop instance, select a suitable option from here.
|*Configuration Entries* |If you want to override the configuration entries of the Hadoop instance, select a suitable option from here.
|===
+
. Access the *Pooling Profile* tab to configure any settings relevant to managing multiple connections using a connection pool.
. Access the *Reconnection* tab to configure any settings relevant to reconnection strategies that Mule should execute if it loses its connection to HDFS.
. Click *Test Connection* to receive a _Connection Successful_ message.
. Click *OK* to save the global connector configurations.
. Return to the Message Flow tab in Studio.

....
[tab,title='XML Editor']
....

First, ensure that you have included the HDFS namespaces in your configuration file.

[source,xml, linenums]
----
<mule xmlns:mulexml="http://www.mulesoft.org/schema/mule/xml" xmlns:hdfs="http://www.mulesoft.org/schema/mule/hdfs"
      xmlns:http="http://www.mulesoft.org/schema/mule/http"
      xmlns:doc="http://www.mulesoft.org/schema/mule/documentation"
      xmlns:spring="http://www.springframework.org/schema/beans"
      xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.mulesoft.org/schema/mule/core"
      xsi:schemaLocation="http://www.mulesoft.org/schema/mule/http http://www.mulesoft.org/schema/mule/http/current/mule-http.xsd
http://www.mulesoft.org/schema/mule/hdfs http://www.mulesoft.org/schema/mule/hdfs/current/mule-hdfs.xsd
http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-current.xsd
http://www.mulesoft.org/schema/mule/core http://www.mulesoft.org/schema/mule/core/current/mule.xsd
http://www.mulesoft.org/schema/mule/xml http://www.mulesoft.org/schema/mule/xml/current/mule-xml.xsd">
----

Follow these steps to configure a HDFS connector in your application:

. Create a global HDFS configuration outside and above your flows, using the following global configuration code.
+
[source,xml]
----
<!-- Simple configuration -->
<hdfs:config nameNodeUri="${mule.HDFS.nameNodeUri}" username="${mule.HDFS.username}"/>
----

[%header,cols="2*a"]
|===
|Parameter |Description
|_Name_ |Enter a name for the configuration with which it can be referenced later by config-ref. The name in this example is `hdfs:config`.
|*nameNode Uri* |Enter the host name or IP address of the master node of the Hadoop cluster.
|*username* |Enter the Hadoop FileSystem username.
|===

....
------

== 使用连接器

HDFS连接器是基于操作的连接器，这意味着将连接器添加到流中时，需要配置连接器执行的特定操作。 HDFS连接器当前支持 http://mulesoft.github.io/mule-hadoop-connector[下面的操作列表]：

[%header,cols="34a,33a,33a"]
|===
|操作 | XML元素 |描述
| *Append to File*  | <hdfs:append>  |将当前有效内容附加到位于指定路径的文件。
| *Copy from Local File*  | <hdfs:copy-from-local-file>  |将本地磁盘上的源文件复制到给定目标路径上的FileSystem。如果应删除源，请设置deleteSource。
| *Copy to Local File*  | <hdfs:copy-to-local-file>  |将文件系统上的源文件复制到给定目标路径的本地磁盘上。如果应删除源，请设置deleteSource。
| *Delete Directory*  | <hdfs:delete-directory>  |删除位于指定路径的文件或目录。
| *Delete File*  | <hdfs:delete-file>  |删除位于指定路径的文件或目录。
| *Get Path Meta Data*  | <hdfs:get-metadata>  |获取路径的元数据并将其存储在流变量中。
| *Glob Status*  | <hdfs:glob-status>  |返回所有匹配文件模式的文件，不是校验和文件。
| *List Status*  | <hdfs:list-status>  |如果路径是目录，列出给定路径中文件和目录的状态。
| *Make Directories*  | <hdfs:make-directories>  |使文件和所有不存在的父目录进入目录。
| *Read from Path*  | <hdfs:read>  |读取由其路径指定的文件的内容，并将内容流式传输到其余的流。还要将HDFS_PATH_EXISTS和HDFS_CONTENT_SUMMARY入站属性添加到路径元数据中。
| *Rename*  | <hdfs:rename>  |将路径目标重命名为路径目标。
| *Set Owner*  | <hdfs:set-owner>  |设置路径的所有者，路径可以是文件或目录的路径。
| *Set Permission*  | <hdfs:set-permission>  |设置路径的权限，路径可以是文件或目录的路径。
| *Write to Path*  | <hdfs:write>  |将当前有效负载写入指定路径，创建新文件或附加到现有文件。
|===

=== 将连接器添加到Mule流中

. 创建一个新的Anypoint Studio项目。
. 添加任何开始的Mule入站端点，例如HTTP侦听器。
. 将HDFS连接器拖到画布上，然后选择它以打开属性编辑器控制台。
. 根据下表配置连接器的参数。
+
image:hdfs_connector_2.png[hdfs_connector_2]
+
[%header,cols="34a,33a,33a"]
|===
| {字段{1}}说明 |缺省
| *Display Name*  |为应用程序中的连接器输入唯一标签。 | HDFS
| *Connector Configuration*  |连接到链接到此连接器的全局元素。全局元素封装有关到目标资源或服务的连接的可重用数据。选择您刚创建的全局HDFS连接器元素。 |
| *Operation*  |选择此组件必须执行的操作。 |
|===
+
. 保存您的配置。

== 用例

以下是HDFS连接器的两种常见用例：

* 使用Mule应用程序在Apache Hadoop实例中创建文件。
* 使用Mule应用程序从Apache Hadoop实例中删除文件。

=== 示例使用案例1

使用Mule应用程序在Hadoop实例中创建文件：

image:hdfsflow.png[hdfsflow]

[tabs]
------
[tab,title="Studio Visual Editor"]
....

. Drag an HTTP connector into the canvas, then select it to open the properties editor console.
. Add a new HTTP Listener Configuration global element:
. In *General Settings*, click the *Add* button:
+
image:4-1.png[4-1]
+
. Configure the following HTTP parameters:
+
image:5-1.png[5-1]
+
[%header,cols="30a,70a"]
|===
|Field|Value
|*Port* |8090
|*Path* |filecreate
|*Host* |localhost
|*Display Name* |HTTP_Listener_Configuration
|===
+
. Reference the HTTP Listener Configuration global element:
+
image:6-1.png[6-1]
+
. Add a Logger scope to print the name of the file that needs to be created in the Mule Console. Configure the Logger according to the table below.
+
image:hdfs-write-to-path-log.png[write to path logger]
+
[%header,cols="30a,70a"]
|===
|Field |Value
|*Display Name* |Write to path log (or any other name you prefer)
|*Message* |Create file:

[source,code]
----
#[message.inboundProperties['http.query.params'].path]
----

With message:

[source,code]
----
#[message.inboundProperties['http.query.params`'].msg]
----

|*Level* |INFO (Default)
|===
+
. Add a Set Payload transformer to set the message input as payload, configuring it according to the table below.
+
[%header,cols="30a,70a"]
|===
|Field |Value
|*Display Name* |Set the message input as payload (or any other name you prefer)
|*Value* |

[source,code]
----
#[message.inboundProperties['http.query.params'].msg]
----

|===
+
. Drag the HDFS connector onto the canvas, and select it to open the properties editor console.
. Click the plus sign next to the *Connector Configuration* field to add a new global connector configuration.
. Configure the global element according to the table below.
+
image:8-1.png[8-1]
+
[%header,cols="30a,70a"]
|===
|Field |Value
|Name |HDFS
|NameNode URI |<NameNode URI of Hadoop instance>
|Username |<Your Hadoop FileSystem username>
|===
+
. Back in the properties editor of the HDFS connector in your application, configure the remaining parameters according to the table below.
+
image:hdfs-write-to-path.png[hdfs-write-to-path]
+
[%header,cols="30a,70a"]
|===
|Field |Value
|*Display Name* |Write to Path (or any other name you prefer)
|*Connector* *Configuration* |hdfs-conf (name of the global element you have created)
|*Operation* |Write to path
|*Path* |

[source,code]
----
#[message.inboundProperties['http.query.params'].path]
----

|===
+
. Run the project as a Mule Application (right-click the project name and click **Run As > Mule Application**).
. From a browser, navigate to `+http://localhost:8090/path=filecreate+`
. Mule conducts the query, and creates the file in Hadoop with the specified message.

....
[tab,title="XML Editor"]
....

. Add an `hdfs:config` global element to your project, and configure its attributes according to the table below.
+
[source,xml]
----
<hdfs:config name="HDFS" doc:name="HDFS" username="username" nameNodeUri="namenode" />
----
+
[%header,cols="30a,70a"]
|===
|Attribute |Value
|*name* |HDFS
|*doc:name* |HDFS
|*username* |<Your Hadoop FileSystem username>
|*nameNodeUri* |NameNode URI of your Hadoop instance
|===
+
. Add an http:listener-config element as shown below.
+
[source,xml, linenums]
----
<http:listener-config name="HTTP_Listener_Configuration" 
host="localhost" port="8090" basePath="filecreate" 
doc:name="HTTP Listener Configuration"/>
<http:connector name="HTTP_HTTPS" cookieSpec="netscape" 
validateConnections="true" 
sendBufferSize="0" 
receiveBufferSize="0" 
receiveBacklog="0" 
clientSoTimeout="10000" 
serverSoTimeout="10000" 
socketSoLinger="0" 
doc:name="HTTP-HTTPS"/>
----
+
[%header,cols="30a,70a"]
|===
|Attribute |Value
|*name* |HTTP_Listener_Configuration
|*host* |localhost
|*port* |8090
|*basePath* |filecreate
|*doc:name* |HTTP Listener Configuration
|===
+
. Begin the flow with an http:listener.
+
[source,xml]
----
<http:listener config-ref="HTTP_Listener_Configuration" path="/" doc:name="HTTP"/>
----
+
[%header,cols="30a,70a"]
|===
|Attribute |Value
|*config-ref* |HTTP_Listener_Configuration
|*Path* |/
|*doc:name* |HTTP
|===
+
. Add a Logger transformer to your flow, configuring the attributes according to the table below.
+
[source,xml]
----
<set-payload value="#[message.inboundProperties['http.query.params'].msg]" doc:name="Set the message input as payload"/>
----
+
[%header,cols="30a,70a"]
|====
|Attribute |Value
|*message* |
[source,code]
----
Creating file: #[message.inboundProperties['http.query.params'].path]
----

[source,code]
----
with message: #[message.inboundProperties['http.query.params'].msg]
----

|*level* |INFO (Default)
|*doc:name* |Write to Path Log
|====
+
. Add a Set Payload transformer to set the message input as payload.
+
[source,xml]
----
<set-payload value="#[message.inboundProperties['http.query.params'].msg]" doc:name="Set the message input as payload"/>
----
+
[%header,cols="30a,70a"]
|======
|Attribute |Value
|*Value* |`#[message.inboundProperties['http.query.params'].msg]`
|*doc:name* |Set the message input as payload
|======
+
. Add a `hdfs:write` element to your flow, configuring the attributes according to the table below.
+
[%header,cols="30a,70a"]
|======
|Attribute |Value
|*config-ref* |hdfs-conf
|*doc:name* |Write to Path
|*path* |`#[message.inboundProperties['http.query.params'].path]`
|======
+
. Run the project as a Mule Application (right-click project name and click *Run As* > *Mule Application*).
. From a browser, navigate to `+http://localhost:8090/path=filecreate+`
. Mule conducts the query, and creates the file in Hadoop with the specified message.

....
------

=== 示例代码

[source,xml, linenums]
----
<mule xmlns:tracking="http://www.mulesoft.org/schema/mule/ee/tracking" xmlns:mulexml="http://www.mulesoft.org/schema/mule/xml" xmlns:hdfs="http://www.mulesoft.org/schema/mule/hdfs"
      xmlns:http="http://www.mulesoft.org/schema/mule/http"
      xmlns:doc="http://www.mulesoft.org/schema/mule/documentation"
      xmlns:spring="http://www.springframework.org/schema/beans"
      xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.mulesoft.org/schema/mule/core"
      xsi:schemaLocation="http://www.mulesoft.org/schema/mule/http http://www.mulesoft.org/schema/mule/http/current/mule-http.xsd
http://www.mulesoft.org/schema/mule/hdfs http://www.mulesoft.org/schema/mule/hdfs/current/mule-hdfs.xsd
http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-current.xsd
http://www.mulesoft.org/schema/mule/core http://www.mulesoft.org/schema/mule/core/current/mule.xsd
http://www.mulesoft.org/schema/mule/xml http://www.mulesoft.org/schema/mule/xml/current/mule-xml.xsd
http://www.mulesoft.org/schema/mule/ee/tracking http://www.mulesoft.org/schema/mule/ee/tracking/current/mule-tracking-ee.xsd">
<hdfs:config name="hdfs-conf" nameNodeUri="Name node URI" username="FileSystem Username" doc:name="HDFS"/>
<http:listener-config name="HTTP_Listener_Configuration" host="localhost" port="8090" basePath="filecreate" doc:name="HTTP Listener Configuration"/>
<http:connector name="HTTP_HTTPS" cookieSpec="netscape" validateConnections="true" sendBufferSize="0" receiveBufferSize="0" receiveBacklog="0" clientSoTimeout="10000" serverSoTimeout="10000" socketSoLinger="0" doc:name="HTTP-HTTPS"/>
<flow name="Create_File_Flow" doc:name="Create_File_Flow">
<http:listener config-ref="HTTP_Listener_Configuration" path="/" doc:name="HTTP"/>
<logger message="Creating file: #[message.inboundProperties['http.query.params'].path] with message: #[message.inboundProperties['http.query.params'].msg]" level="INFO" doc:name="Write to Path Log"/>
<set-payload value="#[message.inboundProperties['http.query.params'].msg]" doc:name="Set the message input as payload"/>
<hdfs:write config-ref="hdfs-conf" path="#[message.inboundProperties['http.query.params'].path]" doc:name="Write to Path"/>
</flow>
</mule>
----

=== 示例使用案例2

使用Mule应用程序从Hadoop实例中删除文件：+
image:DeleteFileFlow.png[DeleteFileFlow]

[tabs]
------
[tab,title="Studio Visual Editor"]
....

. Drag an HTTP connector into the canvas, then select it to open the properties editor console.
. Add a new HTTP Listener Configuration global element:
. In *General Settings*, click the *Add* button:
+
image:11-1.png[11-1]
+
. Configure the following HTTP parameters:
+
image:12-1.png[12-1]
+
[%header,cols="30a,70a"]
|===
|Field|Value
|*Port* |8090
|*Path* |filedelete
|*Host* |localhost
|*Display Name* |HTTP_Listener_Configuration
|===
+
. Reference the HTTP Listener Configuration global element:
+
image:13.png[13]
+
. Add a Logger scope after the HTTP endpoint to print the name of the file that needs to be deleted in the Mule Console. Configure the Logger according to the table below.
+
image:hdfs-delete-file-log.png[hdfs-delete-file-log]
+
[%header,cols="30a,70a"]
|====
|Field |Value
|*Display Name* |Delete file log (or any other name you prefer)
|*Message* |Deleting file:

[source,code]
----
#[message.inboundProperties['http.query.params'].path]
----

|*Level* |INFO (Default)
|====
+
. Drag an HDFS connector onto the canvas, and click it to open the properties editor console.
. Click the plus sign next to the Connector Configuration field to add a new global connector configuration.
. Configure the global element according to the table below.
+
image:15.png[15]
+
[%header,cols="30a,70a"]
|===
|Field |Value
|Names |HDFS
|NameNode URI |<NameNode URI of Hadoop instance>
|Username |<Your Hadoop FileSystem username>
|===
+
. Back in the properties editor of the HDFS connector in your application, configure the remaining parameters according to the table below.
+
image:hdfs-delete-file.png[hdfs-delete-file]
+
[%header,cols="30a,70a"]
|====
|Field |Value
|*Display Name* |Delete file (or any other name you prefer)
|*Connector* *Configuration* |hdfs-conf (name of the global element you have created)
|*Operation* |Delete file
|*Path* |`#[ message.inboundProperties['http.query.params'].path]`
|====
+
. Run the project as a Mule Application (right-click project name, and click *Run As > Mule Application*).
. From a browser, navigate to `+http://localhost:8090/path=filedelete+`
. Mule conducts the query, and deletes the file from Hadoop.

....
[tab,title="XML Editor"]
....

. Add a `hdfs:config` global element to your project, then configure its attributes according to the table below.
+
[source,xml]
----
<hdfs:config name="HDFS" doc:name="HDFS" username="username" nameNodeUri="namenode" />
----
+
[%header,cols="30a,70a"]
|=======
|Attribute |Value
|*name* |HDFS
|*doc:name* |HDFS
|*username* |<Your Hadoop FileSystem username>
|*nameNodeUri* |NameNode URI of your Hadoop instance
|=======
+
. Add a `http:listener-config` element as follows:
+
[source,xml, linenums]
----
<http:listener-config name="HTTP_Listener_Configuration" host="localhost" port="8090" basePath="filedelete" doc:name="HTTP Listener Configuration"/>
<http:connector name="HTTP_HTTPS" cookieSpec="netscape" validateConnections="true" sendBufferSize="0" receiveBufferSize="0" receiveBacklog="0" clientSoTimeout="10000" serverSoTimeout="10000" socketSoLinger="0" doc:name="HTTP-HTTPS"/>
----
+
[%header,cols="30a,70a"]
|====
|Attribute |Value
|*name* |HTTP_Listener_Configuration
|*host* |0.0.0.0
|*port* |8090
|*basePath* |filedelete
|====
+
. Begin the flow with an http:listener statement.
+
[source,xml]
----
<http:listener config-ref="HTTP_Listener_Configuration" path="/" doc:name="HTTP"/>
----
+
. Add a Logger transformer to your flow, configuring the attributes according to the table below.
+
[source,xml, linenums]
----
<logger message="Deleting file:
#[message.inboundProperties['http.query.params'].path]" level="INFO"
doc:name="Delete file log"/>
----
+
[%header,cols="30a,70a"]
|===
|Attribute |Value
|*message* |Deleting file: #`[message.inboundProperties['http.query.params'].path]`
|*level* |INFO (Default)
|*doc:name* |Delete file log
|===
+
. Add an `hdfs:delete-file` element to your flow, configuring the attributes according to the table below.
+
[source,xml]
----
<hdfs:delete-file config-ref="hdfs-conf" doc:name="Delete file" path="#[message.inboundProperties['http.query.params'].path]"/>
----
+
[%header,cols="30a,70a"]
|=======
|Attribute |Value
|*config-ref* |hdfs-conf
|*doc:name* |Delete file
|*path* |`# [message.inboundProperties['http.query.params'].path]`
|=======
+
. Run the project as a Mule Application (right-click project name, then select Run As > Mule Application).
. From a browser, navigate to `+http://localhost:8090/path=filedelete+`
. Mule conducts the query, and deletes the file from Hadoop.

....
------

=== 示例代码
[source,xml, linenums]
----

<mule xmlns:tracking="http://www.mulesoft.org/schema/mule/ee/tracking"
xmlns:mulexml="http://www.mulesoft.org/schema/mule/xml"
xmlns:hdfs="http://www.mulesoft.org/schema/mule/hdfs"
xmlns:http="http://www.mulesoft.org/schema/mule/http"
xmlns:doc="http://www.mulesoft.org/schema/mule/documentation"
xmlns:spring="http://www.springframework.org/schema/beans"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xmlns="http://www.mulesoft.org/schema/mule/core"
xsi:schemaLocation="http://www.mulesoft.org/schema/mule/http
http://www.mulesoft.org/schema/mule/http/current/mule-http.xsd
http://www.mulesoft.org/schema/mule/hdfs
http://www.mulesoft.org/schema/mule/hdfs/current/mule-hdfs.xsd
http://www.springframework.org/schema/beans
http://www.springframework.org/schema/beans/spring-beans-current.xsd
http://www.mulesoft.org/schema/mule/core
http://www.mulesoft.org/schema/mule/core/current/mule.xsd
http://www.mulesoft.org/schema/mule/xml
http://www.mulesoft.org/schema/mule/xml/current/mule-xml.xsd
http://www.mulesoft.org/schema/mule/ee/tracking
http://www.mulesoft.org/schema/mule/ee/tracking/current/mule-tracking-ee.xsd
">
<hdfs:config name="hdfs-conf" nameNodeUri="Name node URI" username="FileSystem Username" doc:name="HDFS"/>
<http:listener-config name="HTTP_Listener_Configuration" host="localhost" port="8090" basePath="filecreate" doc:name="HTTP Listener Configuration"/>
<http:connector name="HTTP_HTTPS" cookieSpec="netscape" validateConnections="true" sendBufferSize="0" receiveBufferSize="0" receiveBacklog="0" clientSoTimeout="10000" serverSoTimeout="10000" socketSoLinger="0" doc:name="HTTP-HTTPS"/>
<flow name="Delete_File_Flow" doc:name="Delete_File_Flow">
<http:listener config-ref="HTTP_Listener_Configuration" path="/" doc:name="HTTP"/>
<logger message="Deleting file:
#[message.inboundProperties['http.query.params'].path]" level="INFO" doc:name="Delete file
log"/>
<hdfs:delete-file config-ref="hdfs-conf" doc:name="Delete file"
path="#[message.inboundProperties['http.query.params'].path]"/>
</flow>
</mule>
----

== 另请参阅

* 详细了解如何使用 link:/mule-user-guide/v/3.7/anypoint-connectors[Anypoint连接器]。
* 访问HDFS连接器 link:/release-notes/hdfs-connector-release-notes[发行说明]。
* 了解如何 link:/mule-user-guide/v/3.7/using-maven-with-mule[使用Male与Mule]。
