= 使用DataMapper处理数据流

在处理大型数据集时特别有用，Anypoint DataMapper支持流式数据输入和输出。例如，当从一个非常大的输入文件读取信息时，可以使用数据流，避免DataMapper将整个文件加载到内存中。相反，DataMapper作为一个流水线工作：它依次读取文件并将数据存储在缓存中，执行数据映射，将输出发送到下一个转换器，清空缓存，然后重新开始。使用此过程，DataMapper可以仅使用大约75 MB的RAM来解析500 MB CSV文件，从而显着提高性能和资源利用率。

*  Anypoint DataMapper流支持CSV和固定宽度的输入和输出格式。
* 您可以配置流缓存的大小以优化性能。

== 假设

本文档假定读者熟悉Anypoint DataMapper Transformer。如果您不是，请从头开始： link:/anypoint-studio/v/6/datamapper-user-guide-and-reference[DataMapper用户指南和参考]。有关DataMapper中所有可用工具的列表，请参阅 link:/mule-user-guide/v/3.4/datamapper-visual-reference[DataMapper可视化参考]。


== 在DataMapper中设置流

. 要设置数据映射流中的*Streaming*参数，请单击DataMapper视图右上方的*Properties*图标以打开*Pattern Properties*窗口。
+
image:properties.png[性能]

. 点击*Streaming*。
. 在*Pipe Size*输入字段中，输入所需的缓存大小。默认值是2048。
* 处理文件时，*Pipe Size*的值以字节表示。
* 处理集合时，值以集合元素的数量表示。
+
image:streaming.png[流]

=== 处理异常

如果映射中发生异常，DataMapper会尽快停止流引擎。为避免发生故障时出现意外后果（例如只将部分行插入数据库），请使用 link:/mule-user-guide/v/3.4/transactions-configuration-reference[交易]。

== 示例

本示例说明了Anypoint DataMapper中Streaming功能的使用。

link:/mule-user-guide/v/3.4/http-endpoint-reference[HTTP端点]接收一个CSV文件，然后将其传递给DataMapper。 DataMapper将输入数据从CSV映射到POJO。 link:/mule-user-guide/v/3.4/database-jdbc-endpoint-reference[数据库（JDBC）端点]将数据插入到外部数据库中。在这种情况下，DataMapper和Database端点作为一个管道并行工作，进一步提高了应用程序的性能。

image:flow2.png[流2]

下图显示了为本例配置的DataMapper视图。

image:dmview2.png[dmview2]

最后，JDBC输出端点接收映射列表，然后将每个项目作为外部数据库的SQL查询中的值。

[source, code, linenums]
----
INSERT INTO Persons (name, city, email, phone) VALUES (#[payload.name], #[payload.city], #[payload.email], #[payload.phone])
----
